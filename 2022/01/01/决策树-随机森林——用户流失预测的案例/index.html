<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>决策树/随机森林——用户流失预测的案例 | ZJH&#39;blog</title>
  <meta name="keywords" content="">
  <meta name="description" content="决策树/随机森林——用户流失预测的案例 | ZJH&#39;blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="决策树决策树是什么 一个根节点，若干个内部节点和叶节点 非参数学习算法 天然的分类器  决策树的目标解决分类和回归问题 原理根据信息熵(entropy) or 基尼系数(gini)的大小决定下一个节点怎么分枝，最后生成决策树，而随机森林就是多个决策树的组合 信息熵entropy超链接：Yjango:什么是信息熵？ 熵在信息论中代表随机变量的不确定性的度量 熵越小，数据不确定性越低熵越大，数据不确定">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树&#x2F;随机森林——用户流失预测的案例">
<meta property="og:url" content="https://zjhblog.gitee.io/2022/01/01/%E5%86%B3%E7%AD%96%E6%A0%91-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A1%88%E4%BE%8B/index.html">
<meta property="og:site_name" content="ZJH&#39;blog">
<meta property="og:description" content="决策树决策树是什么 一个根节点，若干个内部节点和叶节点 非参数学习算法 天然的分类器  决策树的目标解决分类和回归问题 原理根据信息熵(entropy) or 基尼系数(gini)的大小决定下一个节点怎么分枝，最后生成决策树，而随机森林就是多个决策树的组合 信息熵entropy超链接：Yjango:什么是信息熵？ 熵在信息论中代表随机变量的不确定性的度量 熵越小，数据不确定性越低熵越大，数据不确定">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/6a2b7fea48b04223b402fb145f80d793.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16">
<meta property="og:image" content="https://img-blog.csdnimg.cn/30820d3df1b24efb98664f1347d2109d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_19,color_FFFFFF,t_70,g_se,x_16">
<meta property="og:image" content="https://img-blog.csdnimg.cn/16e8639fe87447e1b9f205219401708c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16">
<meta property="og:image" content="https://img-blog.csdnimg.cn/bac26eaf8ba04751a8308ddf86301246.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16">
<meta property="og:image" content="https://img-blog.csdnimg.cn/f0a64f28f5004c82bc2914efde97415c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16">
<meta property="article:published_time" content="2021-12-31T16:41:40.000Z">
<meta property="article:modified_time" content="2022-01-02T06:53:20.033Z">
<meta property="article:author" content="ZJH">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/6a2b7fea48b04223b402fb145f80d793.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16">


<link rel="icon" href="/img/QQ图片20211224224200.jpg">

<link href="/zjh/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/zjh/css/hl_theme/darcula.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/zjh/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/zjh/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 5.4.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="/zjh">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/zjh/"
   class="avatar_target">
    <img class="avatar"
         src="/zjh/img/QQ图片20211224224200.jpg"/>
</a>
<div class="author">
    <span>ZJH</span>
</div>

<div class="icon">
    
        
            <a title="github"
               href="https://github.com/BetterZJH"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-github"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="csdn"
               href="https://blog.csdn.net/m0_56079407"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-csdn"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="email"
               href="mailto:847829122@qq.com"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-email"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="qq"
               href="http://wpa.qq.com/msgrd?v=3&uin=847829122&site=qq&menu=yes"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-qq"></use>
                    </svg>
                
            </a>
        
    
</div>




<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(6)</small>
            
        </div>
    </li>
    
        
            
                <li>
                    <div data-rel="语言">
                        
                        语言
                        <small>(1)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="工具">
                        
                        工具
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="数据结构算法">
                        
                        数据结构算法
                        <small>(2)</small>
                        
                    </div>
                    
                </li>
            
        
    
        
            
                <li>
                    <div data-rel="机器学习">
                        
                        机器学习
                        <small>(1)</small>
                        
                    </div>
                    
                </li>
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
            
    </div>
    <div>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="6">
<input type="hidden" id="yelog_site_word_count" value="9.3k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="http://yelog.org/">叶落阁</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
    </div>

</div>

    
    <nav id="title-list-nav">
        
        <a id="top" class="全部文章 工具 "
           href="/zjh/2021/12/25/IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="IDEA常用快捷键">IDEA常用快捷键</span>
            <span class="post-date" title="2021-12-25 21:55:46">2021/12/25</span>
        </a>
        
        <a id="top" class="全部文章 工具 "
           href="/zjh/2021/12/24/MarkDown%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="MarkDown常用语法">MarkDown常用语法</span>
            <span class="post-date" title="2021-12-24 23:33:22">2021/12/24</span>
        </a>
        
        <a  class="全部文章 机器学习 "
           href="/zjh/2022/01/01/%E5%86%B3%E7%AD%96%E6%A0%91-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E6%B5%81%E5%A4%B1%E9%A2%84%E6%B5%8B%E7%9A%84%E6%A1%88%E4%BE%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="决策树/随机森林——用户流失预测的案例">决策树/随机森林——用户流失预测的案例</span>
            <span class="post-date" title="2022-01-01 00:41:40">2022/01/01</span>
        </a>
        
        <a  class="全部文章 数据结构算法 "
           href="/zjh/2021/12/25/%E5%9B%9E%E5%BD%A2%E9%92%88%E6%95%B0%E5%AD%97%E7%9F%A9%E9%98%B5Java%E5%AE%9E%E7%8E%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="回形针数字矩阵Java实现">回形针数字矩阵Java实现</span>
            <span class="post-date" title="2021-12-25 02:07:12">2021/12/25</span>
        </a>
        
        <a  class="全部文章 语言 "
           href="/zjh/2021/12/25/Java%E5%9B%9B%E7%A7%8Dnext%E7%94%A8%E6%B3%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Java四种next用法">Java四种next用法</span>
            <span class="post-date" title="2021-12-25 01:59:30">2021/12/25</span>
        </a>
        
        <a  class="全部文章 数据结构算法 "
           href="/zjh/2021/12/25/RLE%E7%AE%97%E6%B3%95%E7%9A%84Java%E5%AE%9E%E7%8E%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="RLE算法的Java实现">RLE算法的Java实现</span>
            <span class="post-date" title="2021-12-25 01:35:20">2021/12/25</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-决策树-随机森林——用户流失预测的案例" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">决策树/随机森林——用户流失预测的案例</h1>
    
    <div class="article-meta">
        
        
        
        <span class="book">
            <i class="iconfont icon-category"></i>
            
            
            <a  data-rel="机器学习">机器学习</a>
            
        </span>
        
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2022-01-02 14:53:20'>2022-01-01 00:41</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:2.6k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">决策树是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="toc-text">决策树的目标</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5entropy"><span class="toc-text">信息熵entropy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0"><span class="toc-text">基尼系数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AUC-ROC-tpr-fpr"><span class="toc-text">AUC ROC tpr fpr</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D%EF%BC%9A%E7%94%9F%E6%88%90%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%89%AA%E6%9E%9D"><span class="toc-text">预剪枝：生成决策树的过程中剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D%EF%BC%9A%E7%94%9F%E6%88%90%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="toc-text">后剪枝：生成决策树之后剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%80%BC%EF%BC%9A%E5%8F%AF%E5%8F%96%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-text">连续值：可取连续值的属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%EF%BC%9A%E6%9F%90%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%85%B6%E4%B8%AD%E6%9F%90%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%BC%BA%E5%A4%B1%EF%BC%8C%E8%8B%A5%E5%85%A8%E9%83%A8%E5%BC%83%E7%94%A8%E5%88%99%E4%BC%9A%E5%BE%88%E6%B5%AA%E8%B4%B9"><span class="toc-text">缺失值：某些数据的其中某个数据缺失，若全部弃用则会很浪费</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9A%E7%94%A8%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E6%9B%BF%E4%BB%A3%E5%A4%9A%E4%B8%AA%E5%8F%98%E9%87%8F"><span class="toc-text">多变量决策树：用线性关系替代多个变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-text">随机森林</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-text">实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-text">步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%8C%E5%AF%BC%E5%8C%85"><span class="toc-text">1，导包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%BC%8C%E8%AF%BB%E5%8F%96%E3%80%81%E5%88%9D%E6%AD%A5%E6%9F%A5%E7%9C%8B%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE"><span class="toc-text">2，读取、初步查看分析数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%EF%BC%8C%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-text">3，划分训练集 测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%EF%BC%8C%E5%86%B3%E7%AD%96%E6%A0%91%E5%BB%BA%E6%A8%A1"><span class="toc-text">4，决策树建模</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%EF%BC%9A"><span class="toc-text">网格搜索：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">模型训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-text">模型评估</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%EF%BC%8C%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90"><span class="toc-text">5，决策树生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%8F%82%E6%95%B0%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83"><span class="toc-text">选择最优参数重新训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%98%E5%88%B6%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%8C%E5%9C%A8%E5%90%8C%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%94%9F%E6%88%90pdf"><span class="toc-text">绘制决策树，在同目录下生成pdf</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-1"><span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-text">1,网格搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%AE%AD%E7%BB%83"><span class="toc-text">2,集成学习：随机森林训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%AF%B9%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%EF%BC%8C%E5%B9%B6%E6%B1%82AUC%EF%BC%88%E4%B8%80%E8%88%AC%E9%83%BD%E9%AB%98%E4%BA%8E%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89"><span class="toc-text">3,使用随机森林对结果预测，并求AUC（一般都高于决策树）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="决策树是什么"><a href="#决策树是什么" class="headerlink" title="决策树是什么"></a>决策树是什么</h2><blockquote>
<p>一个根节点，若干个内部节点和叶节点<br><br> 非参数学习算法<br><br> 天然的分类器<br></p>
</blockquote>
<h2 id="决策树的目标"><a href="#决策树的目标" class="headerlink" title="决策树的目标"></a>决策树的目标</h2><p>解决<strong>分类</strong>和<strong>回归</strong>问题</p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>根据<strong>信息熵</strong>(entropy) or <strong>基尼系数</strong>(gini)的大小决定下一个节点怎么分枝，最后生成决策树，而<strong>随机森林</strong>就是多个决策树的组合</p>
<h2 id="信息熵entropy"><a href="#信息熵entropy" class="headerlink" title="信息熵entropy"></a>信息熵entropy</h2><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=yjango%E4%BF%A1%E6%81%AF%E7%86%B5&utm_content=search_suggestion&type=content">超链接：Yjango:什么是信息熵？</a></p>
<p><em>熵在信息论中代表随机变量的不确定性的度量</em></p>
<p>熵越小，数据不确定性越低<br>熵越大，数据不确定性越高</p>
<p>信息熵<br>H=$-\displaystyle \sum^{k}_{i = 1}{Pi}$${log_2{(Pi)}}$</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><ul>
<li>基尼值<br>G = 1 $-\displaystyle \sum^{k}_{i = 1}{Pi^2}$</li>
</ul>
<p>以基尼指数为指标时，应该选择Gini指数最<strong>小</strong>的</p>
<p>CART决策树使用“基尼指数”来选择划分属性</p>
<ul>
<li>  基尼指数<br>G_index = $-\displaystyle \sum^{V}_{v = 1}{\frac{|D^v|}{|D|}Gini(D^v)}$</li>
</ul>
<p><strong>基尼指数到0时，即到叶节点，不能再往下划分</strong></p>
<h2 id="AUC-ROC-tpr-fpr"><a href="#AUC-ROC-tpr-fpr" class="headerlink" title="AUC ROC tpr fpr"></a>AUC ROC tpr fpr</h2><ul>
<li>tpr：Recall，召回率，即当前被分到正样本类别中，真实的正样本占所有正样本的比例，即召回率（召回了多少正样本比例）</li>
<li>fpr：Precision，正例率，即当前划分到正样本类别中，被正确分类的比例（即正式正样本所占比例），就是我们一般理解意义上所关心的正样本的分类准确率；</li>
<li>ROC：tpr和fpr决定的曲线</li>
<li>AUC：ROC曲线下包围的面积，最大值为1，越大拟合性能越好</li>
</ul>
<h2 id="预剪枝：生成决策树的过程中剪枝"><a href="#预剪枝：生成决策树的过程中剪枝" class="headerlink" title="预剪枝：生成决策树的过程中剪枝"></a>预剪枝：生成决策树的过程中剪枝</h2><pre><code>基于“贪心”本质，能剪则剪。
</code></pre>
<p>如果某个分支的存在并没有提高准确率，or降低了准确率，则剪掉</p>
<blockquote>
<p>降低了过拟合风险；<br><br>显著减少了决策树训练时间；<br><br>但带来了欠拟合的风险</p>
</blockquote>
<h2 id="后剪枝：生成决策树之后剪枝"><a href="#后剪枝：生成决策树之后剪枝" class="headerlink" title="后剪枝：生成决策树之后剪枝"></a>后剪枝：生成决策树之后剪枝</h2><pre><code>能不剪，则不剪，剪前后若准确率相等，则保留
</code></pre>
<blockquote>
<p>同样的训练模型，后剪枝的决策树保留了更多的分支<br><br>后剪枝的欠拟合风险很小<br><br>泛化性能往往优于预剪枝(分的更细，在面对陌生数据时判断更准确)<br><br>训练时间长的多(生成决策树之后需要自底向上逐一考察，计算开销大)<br></p>
</blockquote>
<pre><code>“在有噪声的情况下，剪枝操作甚至能将泛化性能提高25%”
</code></pre>
<h2 id="连续值：可取连续值的属性"><a href="#连续值：可取连续值的属性" class="headerlink" title="连续值：可取连续值的属性"></a>连续值：可取连续值的属性</h2><p>例如：脐部{凹陷，平坦，稍凹}这种是离散值；而密度，xx含量等很多属性值都是连续的</p>
<p>因此在划分分支的时候，需要有一个间断点</p>
<p>间断点的划分方法：二分法，例如有17个排序后的点集，两两之间算中位数，一共算16次，生成16个t值，组成一个t的集合T，用T中的划分点代入Gain算法，计算Gain(D，该属性，t为划分点)，取Gain最大值，对应的t即为最终确定的划分点<br>划分完t之后，如果子分支还需用到更细的判断， 可以使用t的子集：例如：一个节点判断“密度&lt;=0.381”那么后续的子节点可以使用任何”密度&lt;0.381”范围的判断依据</p>
<h2 id="缺失值：某些数据的其中某个数据缺失，若全部弃用则会很浪费"><a href="#缺失值：某些数据的其中某个数据缺失，若全部弃用则会很浪费" class="headerlink" title="缺失值：某些数据的其中某个数据缺失，若全部弃用则会很浪费"></a>缺失值：某些数据的其中某个数据缺失，若全部弃用则会很浪费</h2><p>属性a缺失值处理方法：</p>
<p>西瓜书上p88：跳过该属性a的判断，直接判断下一节点的所有可能性，但需要加上训练集中的比例权重</p>
<pre><code>(离散值)：众数填充、相关性最高的列(属性b总是与属性a的取值几乎一一对应)填充
(连续值)：中位数、相关性最高的列(同上)做线性回归估计
</code></pre>
<h2 id="多变量决策树：用线性关系替代多个变量"><a href="#多变量决策树：用线性关系替代多个变量" class="headerlink" title="多变量决策树：用线性关系替代多个变量"></a>多变量决策树：用线性关系替代多个变量</h2><p>有些属性之间有一定的线性关系，例如：密度和含糖量之间存在着线性关系，那么就把密度和含糖量分别乘上各自的权重系数，用他俩组成的一个式子&lt;=t 或 &gt;=t 作为分界点来判断</p>
<pre><code>多变量决策树算法：贪心寻找每个属性的最优权值，线性分类器的最小二乘法
</code></pre>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>擅长于解决数据不平衡的分类</p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="1，导包"><a href="#1，导包" class="headerlink" title="1，导包"></a>1，导包</h3><pre><code>import pandas as pd
import numpy as np
</code></pre>
<h3 id="2，读取、初步查看分析数据"><a href="#2，读取、初步查看分析数据" class="headerlink" title="2，读取、初步查看分析数据"></a>2，读取、初步查看分析数据</h3><pre><code>df = pd.read_csv(&#39;broadband.csv&#39;)
df.rename(str.lower, axis=&#39;columns&#39;, inplace=True)#列名全换小写，方便看

print(df.head())#空参则显示前5行数据
# broadband 即可：0-离开(否)，1-留存(是)
df.info() #输出行列信息（总体数据特征）
print(df.sample()) # 随机查看一个样本数据

# 查看因变量 broadband 分布情况，看是否存在不平衡
from collections import Counter
print(&#39;Broadband: &#39;, Counter(df[&#39;broadband&#39;])) 
    #输出结果是Broadband:  Counter(&#123;0: 131, 1: 49&#125;)，数据并不平衡
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/6a2b7fea48b04223b402fb145f80d793.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16" alt="前5行信息and随机查看样本数据"></p>
<p><img src="https://img-blog.csdnimg.cn/30820d3df1b24efb98664f1347d2109d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_19,color_FFFFFF,t_70,g_se,x_16" alt="info输出信息"></p>
<h3 id="3，划分训练集-测试集"><a href="#3，划分训练集-测试集" class="headerlink" title="3，划分训练集 测试集"></a>3，划分训练集 测试集</h3><p>由于步骤2中info()发现数据集的第一列是用户ID，最后一列是判断标准Broadband，故这两列都不用做数据分析<br>    python<br>    y = df[‘broadband’] # y就是标签(结果)</p>
<pre><code>X = df.iloc[:, 1:-1] # 客户 id 没有用，故丢弃 cust_id；标签y也要去掉，故1：-1
#左边冒号左右端为空，表示所有行数据全部都取到X中

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)
#40%划分为test集，
# 这里的random_state就是为了保证程序每次运行都分割一样的训练集和测试集。
# 否则，同样的算法模型在不同的训练集和测试集上的效果不一样。
# 因此具体取值多少无所谓，但对结果有影响
</code></pre>
<h3 id="4，决策树建模"><a href="#4，决策树建模" class="headerlink" title="4，决策树建模"></a>4，决策树建模</h3><h4 id="网格搜索："><a href="#网格搜索：" class="headerlink" title="网格搜索："></a>网格搜索：</h4><p>因为决策树算法是非参数学习算法，需要自行调参，利用网格搜索则可以自动调参，择优选取<br><br>把自己认为好的参数都扔进去，让网格搜索自己跑</p>
<pre><code>import sklearn.tree as tree

# 1. 直接使用交叉网格搜索来优化决策树模型，边训练边优化
from sklearn.model_selection import GridSearchCV

# 2. 网格搜索参数，选择最优参数,该param_grid作为评价指标用于下面的训练模型
param_grid = &#123;&#39;criterion&#39;: [&#39;entropy&#39;, &#39;gini&#39;], # 树的深度评估指标,信息熵or基尼
            &#39;max_depth&#39;: [2, 3, 4, 5, 6, 7, 8], # 可选树的深度
            &#39;min_samples_split&#39;: [4, 8, 12, 16, 20, 24, 28]&#125; # 可选最小拆分的叶子样本数
</code></pre>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><pre><code># 3. 定义一棵树对象
clf = tree.DecisionTreeClassifier()  

# 4. 传入模型，网格搜索的参数，评估指标，cv交叉验证的次数
clfcv = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=&#39;roc_auc&#39;,cv=4) 
# roc曲线和auc面积值作为评价标准（一般都用auc直接比较面积）
# cv=？表示交叉验证的次数

# 5. 训练模型
clfcv.fit(X_train, y_train)

# 6. 使用模型来对测试集进行预测
test_result = clfcv.predict(X_test)
</code></pre>
<h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><pre><code># 7. 模型评估
import sklearn.metrics as metrics

print(&quot;决策树 AUC:&quot;)
fpr_test, tpr_test, th_test = metrics.roc_curve(y_test, test_result)
#主要是为了得到fpr，tpr，代入AUC计算公式
print(&#39;AUC = %.4f&#39; %metrics.auc(fpr_test, tpr_test))
#输出AUC的值

print(&quot;决策树准确度:&quot;)
print(metrics.classification_report(y_test,test_result))
#输出准确度表格，参考价值不是很大，一般还是以AUC为准


# 9. 求网格搜索后的最优参数
print(clfcv.best_params_)
#输出最优参数组合，但这个最优并非完全最优，可能还需要在开始的地方再重新调参，可能
#算出的最后参数结果还不同
#假设“最小样本设置&#123;4，5，6，7，8&#125;”，而最优结果是4 或 8，处于边缘，
#就需要往边缘调参

#因为该样本结果是：
#&#123;&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 12&#125;
#都不是设置的边缘只，所以可以认为这些参数还不错，暂时不调参
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/16e8639fe87447e1b9f205219401708c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16" alt="决策树AUC" title="决策树AUC"></p>
<h3 id="5，决策树生成"><a href="#5，决策树生成" class="headerlink" title="5，决策树生成"></a>5，决策树生成</h3><h4 id="选择最优参数重新训练"><a href="#选择最优参数重新训练" class="headerlink" title="选择最优参数重新训练"></a>选择最优参数重新训练</h4><pre><code># 将最优参数代入到模型中，重新训练、预测
#&#123;&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 12&#125;

clf2 = tree.DecisionTreeClassifier(criterion=&#39;entropy&#39;, max_depth=3, min_samples_split=12)

clf2.fit(X_train, y_train)
test_res2 = clf2.predict(X_test)
</code></pre>
<h4 id="绘制决策树，在同目录下生成pdf"><a href="#绘制决策树，在同目录下生成pdf" class="headerlink" title="绘制决策树，在同目录下生成pdf"></a>绘制决策树，在同目录下生成pdf</h4><pre><code>#  绘制图形 pip3 install graphviz

import graphviz

dot_data = tree.export_graphviz(clf2, out_file=None)
graph = graphviz.Source(dot_data)
graph.render(&#39;决策树&#39;)#生成文件名为决策树的pdf图片
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/bac26eaf8ba04751a8308ddf86301246.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16" alt="决策树生成" title="决策树生成"></p>
<h2 id="随机森林-1"><a href="#随机森林-1" class="headerlink" title="随机森林"></a>随机森林</h2><p>由于上述决策树生成的AUC值还不到0.7，认为拟合效果不够好，因此尝试使用随机森林算法</p>
<h3 id="1-网格搜索"><a href="#1-网格搜索" class="headerlink" title="1,网格搜索"></a>1,网格搜索</h3><pre><code>param_grid = &#123;
    &#39;criterion&#39;:[&#39;entropy&#39;,&#39;gini&#39;],# 衡量标准
    &#39;max_depth&#39;:[5, 6, 7, 8],    # 每棵决策树的深度
    &#39;n_estimators&#39;:[11,13,15],  # 决策树个数 - 随机森林特有参数
    &#39;max_features&#39;:[0.3,0.4,0.5], # 每棵决策树使用的变量占比 - 随机森林特有参数
    &#39;min_samples_split&#39;:[4,8,12,16]  # 叶子的最小拆分样本量
&#125;
</code></pre>
<h3 id="2-集成学习：随机森林训练"><a href="#2-集成学习：随机森林训练" class="headerlink" title="2,集成学习：随机森林训练"></a>2,集成学习：随机森林训练</h3><pre><code>import sklearn.ensemble as ensemble # ensemble learning: 集成学习

rfc = ensemble.RandomForestClassifier()
rfc_cv = GridSearchCV(estimator=rfc, param_grid=param_grid,
                    scoring=&#39;roc_auc&#39;, cv=4)
rfc_cv.fit(X_train, y_train)
</code></pre>
<h3 id="3-使用随机森林对结果预测，并求AUC（一般都高于决策树）"><a href="#3-使用随机森林对结果预测，并求AUC（一般都高于决策树）" class="headerlink" title="3,使用随机森林对结果预测，并求AUC（一般都高于决策树）"></a>3,使用随机森林对结果预测，并求AUC（一般都高于决策树）</h3><pre><code>predict_test = rfc_cv.predict(X_test)
#训练，预测结束之后，方可查看最佳参数配置
print(rfc_cv.best_params_)

#输出AUC和精度表格
print(&#39;随机森林 AUC...&#39;)
fpr_test, tpr_test, th_test = metrics.roc_curve(predict_test, y_test) 
# 构造 roc 曲线
print(&#39;AUC = %.4f&#39; %metrics.auc(fpr_test, tpr_test))
print(&#39;随机森林精确度...&#39;)
print(metrics.classification_report(predict_test, y_test))
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/f0a64f28f5004c82bc2914efde97415c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-k57uG5Lqa,size_20,color_FFFFFF,t_70,g_se,x_16" alt="随机森林AUC结果"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>调参的思想类似于高中生物实验探究题中的 <strong>“寻找最佳浓度”</strong></p>
<p>衡量决策树 or 随机森林 <strong>模型好坏的标准一般用AUC的值来判断</strong></p>
<p><strong>非参数学习一般都用网格搜索</strong><br><br>例如在进行网格搜索时，有很多参数，哪怕最优结果表明只需要调整一个参数，调整之后其他最优结果可能也会改变（牵一发动全身）<br><em>当然，如果电脑性能足够好，可以直接放很多参数去跑，省去了大量的调参花费的精力</em></p>
<p><strong>网格搜索的参数范围一开始要间隔比较大才好</strong></p>
<p>其他参数相同时，同一个 random_state=？保证了算出来的结果相同</p>

      
       <hr><span style="font-style: italic;color: gray;"> 投其所好，乐此不疲 </span>
    </div>
</article>







    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="//cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<input type="hidden" id="MathJax-js"
        value="//cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML">
</input>
    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2021 ZJH
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

</body>
<script src="/zjh/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/zjh/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        
        $('pre code').each(function(){
            var lines = $(this).text().trim().split('\n').length, widther='';
            if (lines>99) {
                widther = 'widther'
            }
            var $numbering = $('<ul/>').addClass('pre-numbering ' + widther).attr("unselectable","on");
            $(this).addClass('has-numbering ' + widther)
                    .parent()
                    .append($numbering);
            for(var i=1;i<=lines;i++){
                $numbering.append($('<li/>').text(i));
            }
        });
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<style>
    pre{
        position: relative;
        margin-bottom: 24px;
        border-radius: 10px;
        border: 1px solid #e2dede;
        background: #FFF;
        overflow: hidden;
    }
    code.has-numbering{
        margin-left: 30px;
    }
    code.has-numbering.widther{
        margin-left: 35px;
    }
    .pre-numbering{
        margin: 0;
        position: absolute;
        top: 0;
        left: 0;
        width: 20px;
        padding: 0.5em 3px 0.7em 5px;
        border-right: 1px solid #C3CCD0;
        text-align: right;
        color: #AAA;
        background-color: ;
    }
    .pre-numbering.widther {
        width: 35px;
    }
</style>

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
    #post .pjax article :not(pre) > code {
        color: #24292e;
        font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
        background-color: rgba(27,31,35,.05);
        border-radius: 3px;
        font-size: 85%;
        margin: 0;
        padding: .2em .4em;
    }
    
</style>







</html>
