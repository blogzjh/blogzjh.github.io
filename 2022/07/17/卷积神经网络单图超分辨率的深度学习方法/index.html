<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>卷积神经网络单图超分辨率的深度学习方法 | ZJH&#39;blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="摘要我们提出了一种用于单图像超分辨率（SR）的深度学习方法。 我们的方法直接学习低或高分辨率图像之间的端到端映射。 映射被表示为深度卷积神经网络（CNN），其将低分辨率图像作为输入和输出高分辨率。我们进一步表明，还可以将传统的稀疏编码的SR方法视为深卷积网络。 但与传统方法不同，这些方法单独处理每个组件，我们的方法都会共同优化所有层。 我们的深层卷积神经网络具有轻量级结构，但展示了最先进的恢复质量">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络单图超分辨率的深度学习方法">
<meta property="og:url" content="https://blogzjh.github.io/2022/07/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8D%95%E5%9B%BE%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="ZJH&#39;blog">
<meta property="og:description" content="摘要我们提出了一种用于单图像超分辨率（SR）的深度学习方法。 我们的方法直接学习低或高分辨率图像之间的端到端映射。 映射被表示为深度卷积神经网络（CNN），其将低分辨率图像作为输入和输出高分辨率。我们进一步表明，还可以将传统的稀疏编码的SR方法视为深卷积网络。 但与传统方法不同，这些方法单独处理每个组件，我们的方法都会共同优化所有层。 我们的深层卷积神经网络具有轻量级结构，但展示了最先进的恢复质量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/d6f664b731e942f68e464da586d23829.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/6d16c36c60a84707b52124b535d30fd4.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/92c13c1bf2274db7a5759b18ccea5bf5.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/363154deb0e54a16b083f89aee64e2a7.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/8daf3bbc7de1444ab954b4b0148444dd.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/40545e99569d4c94aeaac35694a7ae64.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/9ae717db7900475b8e91774846c4bbea.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/b0901c59b07c40368942b9b1faed0a91.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/8a6539ed46284c5abeaa26324d103740.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/3ac54a07f1964bb1b5abc70a0ce54c30.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/ee4d3708c9b3454ea5e9d8dda8dca498.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/cda8a283a8434982a6fb13bd856e56ad.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/b010c0acd4a448109a2503f49df64dd0.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/682183b46c8a4de7b3fbe4d5fe1efbdd.png">
<meta property="article:published_time" content="2022-07-17T03:33:07.000Z">
<meta property="article:modified_time" content="2022-07-17T03:33:26.870Z">
<meta property="article:author" content="ZJH&#39;blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/d6f664b731e942f68e464da586d23829.png">
  
    <link rel="alternate" href="/atom.xml" title="ZJH&#39;blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ZJH&#39;blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://blogzjh.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-卷积神经网络单图超分辨率的深度学习方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/07/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8D%95%E5%9B%BE%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2022-07-17T03:33:07.000Z" itemprop="datePublished">2022-07-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      卷积神经网络单图超分辨率的深度学习方法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们提出了一种用于单图像超分辨率（SR）的深度学习方法。 我们的方法直接学习低或高分辨率图像之间的端到端映射。 映射被表示为深度卷积神经网络（CNN），其将低分辨率图像作为输入和输出高分辨率。我们进一步表明，还可以将传统的稀疏编码的SR方法视为深卷积网络。 但与传统方法不同，这些方法单独处理每个组件，我们的方法都会共同优化所有层。 我们的深层卷积神经网络具有轻量级结构，但展示了最先进的恢复质量，并实现了实际在线使用的快速速度。 我们探索不同的网络结构和参数设置，以实现性能和速度之间的权衡。 此外，我们将网络扩展到同时应对三种颜色通道，并显示出更好的整体重建质量。</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>We propose a deep learning method for single image hyperresolution (SR). Our method directly learns end-to-end mapping between low- or high resolution images. The mapping is represented as a deep convolutional neural network (CNN), which uses low resolution images as input and output high resolution. We further show that<br>traditional sparsely encoded SR methods can also be regarded as deep convolutional networks. However, unlike traditional methods, these methods deal with each component separately, and our methods will jointly optimise all layers. Our deep convolutional neural network has a lightweight structure, but it shows the most advanced recovery quality and realises the fast speed of actual online use. We explore different network structures and parameter settings to realise the trade off between performance and speed. In addition, we have expanded the network to deal with three colour channels at the same time, and show a better overall reconstruction quality.</p>
<p>目录<br>摘要    2<br>1.前言及研究背景    4<br>1.1图像的超分辨率问题    4<br>1.2传统方法    4<br>1.3 本报告提出的方法    5<br>1.4 优越性    5<br>2.实验过程    5<br>2.1实验方法    5<br>使用深卷积网络进行图像的超级分辨率（SRCNN）：    5<br>2.2 SRCNN的结构特性：    6<br>2.3论证稀疏编码表示在RSCNN中的可行性    7<br>2.4损失函数    8<br>3.实验结果    8<br>3.1调整参数    8<br>3.2评价指标：    9<br>3.3对颜色通道的实验结果：    9<br>4.实验结论    11</p>
<h1 id="1-前言及研究背景"><a href="#1-前言及研究背景" class="headerlink" title="1.前言及研究背景"></a>1.前言及研究背景</h1><h2 id="1-1图像的超分辨率问题"><a href="#1-1图像的超分辨率问题" class="headerlink" title="1.1图像的超分辨率问题"></a>1.1图像的超分辨率问题</h2><p>图像分辨率指图像中存储的信息量，是每英寸图像内有多少个像素点，分辨率的单位为PPI(Pixels Per Inch)，通常叫做像素每英寸。一般情况下，图像分辨率越高，图像中包含的细节就越多，信息量也越大。图像分辨率分为空间分辨率和时间分辨率。通常，分辨率被表示成每一个方向上的像素数量，例如64*64的二维图像。但分辨率的高低其实并不等同于像素数量的多少，例如一个通过插值放大了5倍的图像并不表示它包含的细节增加了多少。图像超分辨率重建关注的是恢复图像中丢失的细节，即高频信息。 在大量的电子图像应用领域，人们经常期望得到高分辨率（简称HR）图像。但由于设备、传感器等原因，我们得到的图像往往是低分辨率图像（LR）。因此,将LR映射到HR的函数空间非常大,学习一个好的解是非常困难的。而且低分辨率图像往往缺失图像超分辨率最需要的高频信息和纹理特征,却包含大量的低频信息来阻碍超分辨率重建,导致图像超分辨率是一个极具挑战性的任务。近年来,深度学习方法在计算机视觉领域大放异彩,同时也给图像超分辨率提供了新的解决方案。【1】增加空间分辨率最直接的解决方法就是通过传感器制造技术减少像素尺寸（例如增加每单元面积的像素数量）；另外一个增加空间分辨率的方法是增加芯片的尺寸，从而增加图像的容量。因为很难提高大容量的偶合转换率，所以这种方法一般不认为是有效的，因此，引出了图像超分辨率技术。<br>针对深度卷积神经网络对特征信息利用不足的问题和基于像素损失的网络无法克服重建图像伪影和平滑的问题进行研究,对深度卷积神经网络的特定结构、网络模块、损失函数等进行分析,通过构建信息蒸馏和密集特征融合模块加强了网络内部信息的流动效率;引入改进的生成对抗网络来改善重建图像的伪影和过于平滑的问题。【2】<br>超分辨率问题本质是不适定的或者说欠定逆问题，也就是图像超分辨率的解是不唯一的。对于任何给定的低分辨率像素都存在多重解，所以对于这个问题通常用强先验信息约束解空间来缓解，为了学习强先验信息，主要有两种思路：<br>（1）利用同一图像的内部相似性；<br>（2）从外部低分辨率和高分辨率样例图片对中学习映射函数</p>
<h2 id="1-2传统方法"><a href="#1-2传统方法" class="headerlink" title="1.2传统方法"></a>1.2传统方法</h2><p>传统基于稀疏编码的超分辨率方法是基于外部实例方法的一种方法，该方法流程如下：<br>（1）从输入图片中密集地进行采样形成大量的重叠的patches，并且对这些patch进行预处理（例如减去均值、标准化）；<br>（2）用一个低分辨率的字典对patches进行编码；<br>（3）稀疏系数被传递到一个高分辨率字典中，用来重构高分辨率的patches；<br>（4）将重叠的patches进行聚合（平均权重）产生最终的输出；</p>
<h2 id="1-3-本报告提出的方法"><a href="#1-3-本报告提出的方法" class="headerlink" title="1.3 本报告提出的方法"></a>1.3 本报告提出的方法</h2><p>这篇课程设计报告的实验是基于2015年由何恺明博士、汤晓鸥教授等发表于TPAMI的关于图像超分辨率的一篇论文《Image Super-resolution Using Deep Convolutional Networks (SRCNN)》，同时也是世界上较早的将深度学习应用在图像超分辨率的工作【3】，文中提出了一种用于单图像超分辨率的三层卷积网络，即SRCNN来实现图像的超分辨率的深度学习方法，相较于传统的基于稀疏表示的字典学习方法，一定程度上提升了图像重建的质量。</p>
<h2 id="1-4-优越性"><a href="#1-4-优越性" class="headerlink" title="1.4 优越性"></a>1.4 优越性</h2><p>针对低分辨率图像在预处理时使用双三次插值导致图像丢失一些重要的高频纹理细节以及网络模型优化问题,文章提出了连分式插值结合卷积神经网络的超分辨率重建方法。在原有的轻量级基于卷积神经网络的超分辨率重建算法（super-resolution convolutional neural net work, SRCNN）网络模型基础上,首先采用Newton-Thiele型连分式插值函数将低分辨率图像插值到目标尺寸;然后利用3个卷积层进行图像特征提取、非线性映射、重建与优化;该文在网络收敛时利用Radam优化算法自适应地调整梯度,并且采用余弦衰减法逐渐降低学习率。实验结果表明,该网络模型能够在轻量级的卷积神经网络下获得更丰富的纹理细节和更清晰的图像边缘。 【4】<br>与稀疏编码表示的方法相比，SRCNN有诸多优点：<br>①SRCNN没有显式地学习用于modeling the patch space的dictionaries或manifolds，而是通过隐藏层隐式实现的。<br>②patch的提取和聚合也被表示为卷积层，因此也被包含在了优化过程中。<br>③完全是通过学习获得的，几乎没有预处理或后处理。<br>④结构简单而且准确率还比一些基于实例的先进方法的高；<br>⑤有合适的卷积核数和层数，即便使用CPU，也能有很快的速度；<br>⑥SRCNN是完全前馈的网络，在使用中不需要解决任何优化问题，因此要快于其他基于实例的方法；<br>⑦当数据集更大、更多样化或者模型更大、更深时，网络的恢复质量可以进一步提高；<br>⑧SRCNN可以同时处理彩色图像的三通道，提高了超分辨率性能</p>
<h1 id="2-实验过程"><a href="#2-实验过程" class="headerlink" title="2.实验过程"></a>2.实验过程</h1><h2 id="2-1实验方法"><a href="#2-1实验方法" class="headerlink" title="2.1实验方法"></a>2.1实验方法</h2><p>使用深卷积网络进行图像的超级分辨率（SRCNN）：<br>简单来讲，SRCNN的原理可以概括为：直接学习低/高分辨率图像之间的端到端映射。 这一映射被表示为深度卷积神经网络（CNN），其将低分辨率图像作为输入，经映射后输出高分辨率图形，达到了很好的效果，同时证明了传统基于稀疏编码的SR方法也可以看作是一个深度卷积网络。其区别在于，传统方法是分别处理每个组件，而SRCNN则是联合优化所有层。SRCNN网络问世后,它在图像超分辨重建的应用使得深度学习在图像处理领域得到扩展。2014年,深度学习模型SRCNN一经提出,为深度学习解决图像压缩领域的像素问题开创了新纪元。【5】</p>
<p><img src="https://img-blog.csdnimg.cn/d6f664b731e942f68e464da586d23829.png" alt="图1 论文中展示的三种方法效果比较"><br><img src="https://img-blog.csdnimg.cn/6d16c36c60a84707b52124b535d30fd4.png" alt="图2 用demo code复现进行图片测试SRCNN的超分辨率效果"></p>
<h2 id="2-2-SRCNN的结构特性："><a href="#2-2-SRCNN的结构特性：" class="headerlink" title="2.2 SRCNN的结构特性："></a>2.2 SRCNN的结构特性：</h2><p>SRCNN具有三层网络结构，在具有轻量结构的同时，还具有最先进的恢复质量，并实现了快速的在线使用，作者在论文中探索了不同的网络结构和参数设置，以实现性能和速度之间的权衡。从处理过程来看，SRCNN可以同时处理RGB三个颜色通道，从而表现出更好的整体重建质量。<br><img src="https://img-blog.csdnimg.cn/92c13c1bf2274db7a5759b18ccea5bf5.png" alt="图3 SRCNN的三层网络结构图解"></p>
<ul>
<li><p>第一卷积层：提取一组特征映射<img src="https://img-blog.csdnimg.cn/363154deb0e54a16b083f89aee64e2a7.png"><br>W1表示为滤波器，B1表示为偏差；</p>
</li>
<li><p>第二卷积层：映射这些特征非线性地映射到高分辨率表示<img src="https://img-blog.csdnimg.cn/8daf3bbc7de1444ab954b4b0148444dd.png"></p>
</li>
<li><p>第三卷积层：结合了空间邻域内的预测重建，以产生最终的高分辨率图像<img src="https://img-blog.csdnimg.cn/40545e99569d4c94aeaac35694a7ae64.png"><br>W3是一组线性滤波器</p>
</li>
</ul>
<h2 id="2-3论证稀疏编码表示在RSCNN中的可行性"><a href="#2-3论证稀疏编码表示在RSCNN中的可行性" class="headerlink" title="2.3论证稀疏编码表示在RSCNN中的可行性"></a>2.3论证稀疏编码表示在RSCNN中的可行性</h2><p><img src="https://img-blog.csdnimg.cn/9ae717db7900475b8e91774846c4bbea.png" alt="图4 基于稀疏编码的方法的例证在卷积神经网络的视图中"><br>①左半部分：可以看作是把f1×f1低分辨率的patch看成是从输入图片提取出来的，稀疏编码将patch投影到一个字典上，如果这个字典的大小为n1，那么相当于在输入图片上使用n1个大小f1×f1的卷积核进行卷积运算。<br>②中间部分：稀疏编码会迭代的处理n1维度的向量，从而得到一个n2维度的向量，一般来说n1、n2是相等的，这个时候稀疏编码求解程序起的作用就是一个大小为1×1的非线性映射运算符。<br>③右半部分：最后再对n2维度的向量投影到另一个字典空间目的是产生高分辨率的patch，重叠部分的patch会进行平均操作。</p>
<h2 id="2-4损失函数"><a href="#2-4损失函数" class="headerlink" title="2.4损失函数"></a>2.4损失函数</h2><p>SRCNN模型的参数有：W1, W2, W3, B1, B2, B3。给定一组高分辨率图片Xi以及对应低分辨率图片Yi，我们使用均方误差MSE（mean squared error）作为损失函数。设为：<img src="https://img-blog.csdnimg.cn/b0901c59b07c40368942b9b1faed0a91.png"><br>其中n是训练样本的数量。<br>损失函数使用随机梯度下降进行优化<br><img src="https://img-blog.csdnimg.cn/8a6539ed46284c5abeaa26324d103740.png"><br>其中的l∈{1，2，3}，i是层和迭代的索引，每层的卷积核权重都由均值为0标准差为0.001的高斯分布进行初始化，每层偏置都初始化为0，η是学习率，前两层的学习率为10^-4 ，最后一层的学习率为10^-5，尤其最后一层使用小的学习率对SRCNN的收敛性能影响较大。</p>
<p>可见，四层网络收敛的比三层的慢，但若有足够的训练时间，四层网络最终也能赶上三层的网络。论文中实验表明网络并不是越深越好，当使用越深的网络（例如四层或五层的网络），我们发现很难去设置一个合适的学习率来确保网络收敛，即使模型收敛了，也可能陷入一个不好的局部最小值，并且经过足够的训练时间，已学习的卷积核的多样性会变少。而且在图像分类领域，不适当的增加模型深度也会使得准确率的下降或退化。</p>
<h1 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h1><h2 id="3-1调整参数"><a href="#3-1调整参数" class="headerlink" title="3.1调整参数"></a>3.1调整参数</h2><p>为了平衡性能和速度，我们的方法选用了f1=9, f2=5, f3=5, n1=64, n2=32,并且在ImageNet上进行训练。并且对每个放大系数∈{2, 3, 4}，都分别训练了一个相应的网络。</p>
<p><img src="https://img-blog.csdnimg.cn/3ac54a07f1964bb1b5abc70a0ce54c30.png" alt="图5 SET5数据集上的PSNR，SSIM，IFC，NQM，WPSNR和MSSIM的平均结果"></p>
<h2 id="3-2评价指标："><a href="#3-2评价指标：" class="headerlink" title="3.2评价指标："></a>3.2评价指标：</h2><p>①PSNR（Peak Signal to Noise Ratio）峰值信噪比：PSNR是最普遍，最广泛使用的评鉴画质的客观量测法。但这个指标只是速度快，评价效果一般；<br>②SSIM（Structural SIMilarity）结构相似性：是一种衡量两幅图像相似度的指标，用均值作为亮度的估计，标准差作为对比度的估计，协方差作为结构相似程度的度量。<br>③IFC（information fidelity criterion）信息保真度准则：通过计算待评图像与参考图像之间的互信息来衡量待评图像的质量优劣。<br>④NQM（noise quality measure）噪声质量测量<br>⑤WPSNR(Weighted Peak Signal to Noise Ratio)加权峰值信噪比<br>⑥MSSSIM（Multi Scale Structural SIMilarity ）多尺度结构相似性</p>
<h2 id="3-3对颜色通道的实验结果："><a href="#3-3对颜色通道的实验结果：" class="headerlink" title="3.3对颜色通道的实验结果："></a>3.3对颜色通道的实验结果：</h2><p><img src="https://img-blog.csdnimg.cn/ee4d3708c9b3454ea5e9d8dda8dca498.png" alt="图6 SET5数据集上的不同通道的平均PSNR（DB）和培训策略"></p>
<h1 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h1><p>①Y only：基线方法，是一个单通道网络（c=1），只在亮度上进行了训练。对Cb、Cr通道采用双三次插值进行了扩展。<br>②YCbCr：在YCbCr空间的三个通道上进行训练<br>③Y pre-train ：使用Y通道的MSE作为损失来对网络进行预训练。然后利用各通道的MSE对参数进行微调<br>④CbCr pre-train：使用Cb,Cr通道的MSE作为损失来对网络进行预训练，然后对所有通道的参数进行微调<br>⑤RGB：在RGB空间的三个通道上进行训练<br><img src="https://img-blog.csdnimg.cn/cda8a283a8434982a6fb13bd856e56ad.png" alt="图7 具有不同图层的映射"><br><img src="https://img-blog.csdnimg.cn/b010c0acd4a448109a2503f49df64dd0.png"><br><img src="https://img-blog.csdnimg.cn/682183b46c8a4de7b3fbe4d5fe1efbdd.png"></p>
<h1 id="4-实验结论"><a href="#4-实验结论" class="headerlink" title="4.实验结论"></a>4.实验结论</h1><p>通过这篇论文，学习了SRCNN这种对于单图像进行超分辨率的深度学习方，通过公式演算，一步步探寻到了传统的基于稀疏编码的SR方法可以重构为一个深度卷积神经网络。我们表明，传统的基于稀疏编码的SR方法可以被重新表述为一个深度卷积神经网络。所提出的方法，即SRCNN，可以学习低分辨率和高分辨率图像之间的端到端映射，除了优化之外几乎没有额外的前/后处理。凭借轻量级的结构，SRCNN已经取得了比最先进的方法更出色的性能。我们猜想，通过探索更多的滤波器和不同的训练策略，可以进一步获得额外的性能。此外，所提出的结构具有简单和稳健的优点，可以应用于其他低层次的视觉问题，如图像去模糊化或同步SR+去噪。我们还可以研究一个网络来应对不同的放大系数。<br>SRCNN学习低分辨率和高分辨率图像之间的端到端映射，除了优化之外几乎没有额外的预处理/后处理，由于采用了轻型结构，SRCNN的性能优于目前基于实例的的方法。并进一步探索可以知道更多的卷积核和不同的训练策略，可以获得更好的性能。对于一些低质图片，可以有很大的发挥作用的空间，在人脸识别等领域大展身手。</p>
<h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p>[1]张俊韬. 基于注意力卷积神经网络的图像超分辨率研究[D].哈尔滨工程大学,2021.DOI:10.27060/d.cnki.ghbcu.2021.001483.<br>[2]查体博. 基于深度卷积神经网络的列车检测图像超分辨方法研究[D].西南交通大学,2021.DOI:10.27414/d.cnki.gxnju.2021.002909.<br>[3]Chao Dong,Chen Change Loy,Kaiming He,Xiaoou Tang. Image Super-Resolution Using Deep Convolutional Networks.[J]. CoRR,2015,abs/1501.00092.<br>[4]杨悦,谢辛,何蕾,胡敏.连分式插值结合卷积神经网络的超分辨率重建[J].合肥工业大学学报(自然科学版),2021,44(08):1146-1152.<br>[5]曾诗悦.基于SRCNN模型的图像压缩方法研究[J].信息技术与信息化,2020(09):98-100.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://blogzjh.github.io/2022/07/17/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8D%95%E5%9B%BE%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" data-id="cl8wv0v2u00251gta5doi2dui" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/07/17/%E6%B0%94%E8%B1%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          气象大数据分析
        
      </div>
    </a>
  
  
    <a href="/2022/07/17/%E7%96%AB%E6%83%85%E6%B8%AF%E5%8F%A3%E6%8C%82%E9%9D%A0%E6%95%B0ARIMA%E6%97%B6%E5%BA%8F%E5%88%86%E6%9E%90/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">疫情港口挂靠数ARIMA时序分析</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/">中间件</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E4%BD%9C%E7%AC%94%E8%AE%B0/">工作笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95/">数据结构算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%BA%90%E7%A0%81/">源码</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/HashMap/" rel="tag">HashMap</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/HashMap/" style="font-size: 10px;">HashMap</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">九月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">七月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/09/26/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%81%9A%E9%BB%91%E5%90%8D%E5%8D%95%E7%9A%84%E6%96%B9%E6%A1%88/">布隆过滤做黑名单的方案</a>
          </li>
        
          <li>
            <a href="/2022/09/15/%E6%9F%90%E8%9A%81%E9%87%91%E6%9C%8D%E7%A7%8B%E6%8B%9B9-15/">某蚁金服秋招9-15</a>
          </li>
        
          <li>
            <a href="/2022/09/15/%E4%B8%89%E7%A7%8D%E6%83%85%E5%86%B5%E7%9A%84%E5%B1%82%E5%BA%8F%E9%81%8D%E5%8E%86/">三种情况的层序遍历</a>
          </li>
        
          <li>
            <a href="/2022/09/15/%E5%87%A0%E9%81%93%E5%B2%9B%E5%B1%BF%E9%97%AE%E9%A2%98-dfs-%E5%89%AA%E6%9E%9D-%E5%9B%9E%E6%BA%AF/">几道岛屿问题:dfs+剪枝+回溯</a>
          </li>
        
          <li>
            <a href="/2022/09/15/%E5%AD%97%E7%AC%A6%E4%B8%B2%EF%BC%9A%E6%9B%BF%E6%8D%A2%E3%80%81%E5%B7%A6%E6%97%8B%E3%80%81%E7%BC%BA%E5%A4%B1/">字符串：替换、左旋、缺失</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 ZJH&#39;blog<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>